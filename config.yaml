# Vision Transformer Configuration for CIFAR-10

# Data settings
data:
  dataset: "cifar10"
  data_dir: "E:/PROJECT/vit_classifier/data"
  num_classes: 10
  image_size: 72
  batch_size: 256
  num_workers: 4
  val_split: 0.1

# Model architecture
model:
  patch_size: 6
  projection_dim: 64
  num_heads: 4
  transformer_layers: 8
  transformer_units: [128, 64]  # [projection_dim*2, projection_dim]
  mlp_head_units: [2048, 1024]
  dropout_rate: 0.1
  mlp_dropout_rate: 0.5

# Training settings
training:
  num_epochs: 100
  learning_rate: 0.001
  weight_decay: 0.0001
  warmup_epochs: 10
  gradient_clip: 1.0
  mixed_precision: true
  
# Augmentation
augmentation:
  random_flip: true
  random_rotation: 7.2  # degrees
  random_crop_scale: [0.8, 1.0]
  color_jitter: false
  normalize_mean: [0.485, 0.456, 0.406]
  normalize_std: [0.229, 0.224, 0.225]

# Optimization
optimizer:
  type: "adamw"  # adamw, adam, sgd
  momentum: 0.9  # for SGD
  
scheduler:
  type: "cosine"  # cosine, step, plateau, none
  step_size: 30  # for step scheduler
  gamma: 0.1  # for step scheduler
  patience: 10  # for plateau scheduler

# Checkpointing
checkpoint:
  save_dir: "E:/PROJECT/vit_classifier/results/checkpoints"
  save_best_only: true
  monitor: "val_accuracy"
  mode: "max"
  
# Logging
logging:
  log_dir: "E:/PROJECT/vit_classifier/results/logs"
  tensorboard: true
  print_freq: 50
  save_plots: true
  plot_dir: "E:/PROJECT/vit_classifier/results/plots"

# Reproducibility
seed: 42