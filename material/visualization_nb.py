{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer Results Visualization\n",
    "\n",
    "This notebook visualizes training results, model predictions, and attention maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from model import create_model\n",
    "from data import get_dataloaders, get_class_names, denormalize\n",
    "from utils import load_config, load_checkpoint\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('../config.yaml')\n",
    "\n",
    "# Load dataloaders\n",
    "train_loader, val_loader, test_loader = get_dataloaders(config)\n",
    "class_names = get_class_names()\n",
    "\n",
    "# Create model\n",
    "model = create_model(config).to(device)\n",
    "\n",
    "# Load best checkpoint\n",
    "checkpoint_path = '../results/checkpoints/checkpoint_epoch_1_best.pth'\n",
    "if Path(checkpoint_path).exists():\n",
    "    load_checkpoint(checkpoint_path, model)\n",
    "    print(\"Loaded best model checkpoint\")\n",
    "else:\n",
    "    print(\"Warning: No checkpoint found. Using untrained model.\")\n",
    "\n",
    "model.eval()\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Predictions Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of test images\n",
    "images, labels = next(iter(test_loader))\n",
    "images_gpu = images.to(device)\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(images_gpu)\n",
    "    probabilities = torch.softmax(outputs, dim=1)\n",
    "    confidences, predictions = torch.max(probabilities, 1)\n",
    "\n",
    "predictions = predictions.cpu()\n",
    "confidences = confidences.cpu()\n",
    "\n",
    "# Visualize predictions\n",
    "num_images = 16\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx in range(num_images):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Denormalize image\n",
    "    img = denormalize(\n",
    "        images[idx],\n",
    "        config['augmentation']['normalize_mean'],\n",
    "        config['augmentation']['normalize_std']\n",
    "    )\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "    \n",
    "    # Plot image\n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "    \n",
    "    # Add title with prediction\n",
    "    true_label = class_names[labels[idx]]\n",
    "    pred_label = class_names[predictions[idx]]\n",
    "    confidence = confidences[idx].item() * 100\n",
    "    \n",
    "    color = 'green' if labels[idx] == predictions[idx] else 'red'\n",
    "    ax.set_title(\n",
    "        f'True: {true_label}\\nPred: {pred_label} ({confidence:.1f}%)',\n",
    "        color=color,\n",
    "        fontsize=10,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/model_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy on this batch\n",
    "correct = (predictions[:num_images] == labels[:num_images]).sum().item()\n",
    "print(f\"\\nBatch Accuracy: {correct}/{num_images} = {100*correct/num_images:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect predictions on entire test set\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"Evaluating on test set...\")\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    cbar_kws={'label': 'Count'},\n",
    "    square=True\n",
    ")\n",
    "plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=14, fontweight='bold')\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=16, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/detailed_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy = 100 * np.sum(np.array(all_predictions) == np.array(all_labels)) / len(all_labels)\n",
    "print(f\"\\nTest Set Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Per-Class Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "report = classification_report(\n",
    "    all_labels,\n",
    "    all_predictions,\n",
    "    target_names=class_names,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report = df_report.iloc[:-3]  # Remove avg rows for visualization\n",
    "\n",
    "# Plot per-class metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics = ['precision', 'recall', 'f1-score']\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "\n",
    "for idx, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    ax = axes[idx]\n",
    "    values = df_report[metric].values\n",
    "    bars = ax.bar(class_names, values, color=color, edgecolor='black', alpha=0.7)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width()/2.,\n",
    "            height,\n",
    "            f'{height:.2f}',\n",
    "            ha='center',\n",
    "            va='bottom',\n",
    "            fontsize=9\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(metric.capitalize(), fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{metric.capitalize()} per Class', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/per_class_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print detailed report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(df_report.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Confidence Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect prediction confidences\n",
    "correct_confidences = []\n",
    "incorrect_confidences = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        confidences, predictions = torch.max(probabilities, 1)\n",
    "        \n",
    "        # Separate correct and incorrect predictions\n",
    "        correct_mask = predictions.cpu() == labels\n",
    "        correct_confidences.extend(confidences[correct_mask].cpu().numpy())\n",
    "        incorrect_confidences.extend(confidences[~correct_mask].cpu().numpy())\n",
    "\n",
    "# Plot distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(correct_confidences, bins=50, alpha=0.7, label='Correct', color='green', edgecolor='black')\n",
    "axes[0].hist(incorrect_confidences, bins=50, alpha=0.7, label='Incorrect', color='red', edgecolor='black')\n",
    "axes[0].set_xlabel('Confidence', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=12)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "data_to_plot = [correct_confidences, incorrect_confidences]\n",
    "bp = axes[1].boxplot(data_to_plot, labels=['Correct', 'Incorrect'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('lightgreen')\n",
    "bp['boxes'][1].set_facecolor('lightcoral')\n",
    "axes[1].set_ylabel('Confidence', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Confidence Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/confidence_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCorrect predictions - Mean confidence: {np.mean(correct_confidences):.3f}\")\n",
    "print(f\"Incorrect predictions - Mean confidence: {np.mean(incorrect_confidences):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Attention Map Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single image\n",
    "images, labels = next(iter(test_loader))\n",
    "image = images[0:1].to(device)\n",
    "label = labels[0]\n",
    "\n",
    "# Get attention maps\n",
    "with torch.no_grad():\n",
    "    logits, attention_weights = model(image, return_attention=True)\n",
    "    prediction = torch.argmax(logits, dim=1)\n",
    "\n",
    "print(f\"Number of attention layers: {len(attention_weights)}\")\n",
    "print(f\"Attention shape per layer: {attention_weights[0].shape}\")\n",
    "\n",
    "# Visualize attention from different layers\n",
    "num_layers_to_show = min(8, len(attention_weights))\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Get number of patches\n",
    "num_patches = attention_weights[0].shape[-1]\n",
    "patch_per_dim = int(np.sqrt(num_patches))\n",
    "\n",
    "for layer_idx in range(num_layers_to_show):\n",
    "    ax = axes[layer_idx]\n",
    "    \n",
    "    # Get attention for this layer (average across heads and query tokens)\n",
    "    attn = attention_weights[layer_idx][0]  # Remove batch dimension\n",
    "    attn_avg = attn.mean(dim=0).mean(dim=0)  # Average across heads and queries\n",
    "    \n",
    "    # Reshape to 2D grid\n",
    "    attn_map = attn_avg.cpu().reshape(patch_per_dim, patch_per_dim)\n",
    "    \n",
    "    # Plot\n",
    "    im = ax.imshow(attn_map, cmap='viridis', interpolation='nearest')\n",
    "    ax.set_title(f'Layer {layer_idx + 1}', fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.suptitle(\n",
    "    f'Attention Maps - True: {class_names[label]}, Pred: {class_names[prediction[0]]}',\n",
    "    fontsize=16,\n",
    "    fontweight='bold',\n",
    "    y=1.02\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/attention_maps.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Attention Overlay on Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few images to visualize\n",
    "num_samples = 6\n",
    "images, labels = next(iter(test_loader))\n",
    "images_gpu = images[:num_samples].to(device)\n",
    "\n",
    "fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4*num_samples))\n",
    "\n",
    "for img_idx in range(num_samples):\n",
    "    image = images_gpu[img_idx:img_idx+1]\n",
    "    label = labels[img_idx]\n",
    "    \n",
    "    # Get prediction and attention\n",
    "    with torch.no_grad():\n",
    "        logits, attention_weights = model(image, return_attention=True)\n",
    "        prediction = torch.argmax(logits, dim=1)\n",
    "    \n",
    "    # Original image\n",
    "    img_display = denormalize(\n",
    "        images[img_idx],\n",
    "        config['augmentation']['normalize_mean'],\n",
    "        config['augmentation']['normalize_std']\n",
    "    )\n",
    "    img_display = torch.clamp(img_display, 0, 1).permute(1, 2, 0).numpy()\n",
    "    \n",
    "    # Plot original\n",
    "    axes[img_idx, 0].imshow(img_display)\n",
    "    axes[img_idx, 0].set_title('Original Image', fontsize=12, fontweight='bold')\n",
    "    axes[img_idx, 0].axis('off')\n",
    "    \n",
    "    # Early layer attention (layer 2)\n",
    "    attn_early = attention_weights[1][0].mean(dim=0).mean(dim=0).cpu()\n",
    "    attn_early_map = attn_early.reshape(patch_per_dim, patch_per_dim)\n",
    "    attn_early_resized = torch.nn.functional.interpolate(\n",
    "        attn_early_map.unsqueeze(0).unsqueeze(0),\n",
    "        size=(config['data']['image_size'], config['data']['image_size']),\n",
    "        mode='bilinear'\n",
    "    ).squeeze().numpy()\n",
    "    \n",
    "    axes[img_idx, 1].imshow(img_display)\n",
    "    axes[img_idx, 1].imshow(attn_early_resized, cmap='jet', alpha=0.5)\n",
    "    axes[img_idx, 1].set_title('Early Layer Attention (Layer 2)', fontsize=12, fontweight='bold')\n",
    "    axes[img_idx, 1].axis('off')\n",
    "    \n",
    "    # Late layer attention (last layer)\n",
    "    attn_late = attention_weights[-1][0].mean(dim=0).mean(dim=0).cpu()\n",
    "    attn_late_map = attn_late.reshape(patch_per_dim, patch_per_dim)\n",
    "    attn_late_resized = torch.nn.functional.interpolate(\n",
    "        attn_late_map.unsqueeze(0).unsqueeze(0),\n",
    "        size=(config['data']['image_size'], config['data']['image_size']),\n",
    "        mode='bilinear'\n",
    "    ).squeeze().numpy()\n",
    "    \n",
    "    axes[img_idx, 2].imshow(img_display)\n",
    "    axes[img_idx, 2].imshow(attn_late_resized, cmap='jet', alpha=0.5)\n",
    "    \n",
    "    color = 'green' if prediction[0] == label else 'red'\n",
    "    axes[img_idx, 2].set_title(\n",
    "        f'Late Layer Attention (Layer 8)\\nTrue: {class_names[label]}, Pred: {class_names[prediction[0]]}',\n",
    "        fontsize=12,\n",
    "        fontweight='bold',\n",
    "        color=color\n",
    "    )\n",
    "    axes[img_idx, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/attention_overlay.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Most Confident Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most confident correct and incorrect predictions\n",
    "all_images = []\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "all_confidences = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images_gpu = images.to(device)\n",
    "        outputs = model(images_gpu)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        confidences, predictions = torch.max(probs, 1)\n",
    "        \n",
    "        all_images.append(images)\n",
    "        all_labels.append(labels)\n",
    "        all_predictions.append(predictions.cpu())\n",
    "        all_confidences.append(confidences.cpu())\n",
    "\n",
    "all_images = torch.cat(all_images)\n",
    "all_labels = torch.cat(all_labels)\n",
    "all_predictions = torch.cat(all_predictions)\n",
    "all_confidences = torch.cat(all_confidences)\n",
    "\n",
    "# Find top confident correct predictions\n",
    "correct_mask = all_predictions == all_labels\n",
    "correct_confidences = all_confidences[correct_mask]\n",
    "correct_indices = torch.where(correct_mask)[0]\n",
    "top_correct_indices = correct_indices[torch.argsort(correct_confidences, descending=True)[:8]]\n",
    "\n",
    "# Find top confident incorrect predictions\n",
    "incorrect_mask = all_predictions != all_labels\n",
    "incorrect_confidences = all_confidences[incorrect_mask]\n",
    "incorrect_indices = torch.where(incorrect_mask)[0]\n",
    "top_incorrect_indices = incorrect_indices[torch.argsort(incorrect_confidences, descending=True)[:8]]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 8, figsize=(20, 6))\n",
    "\n",
    "# Top correct\n",
    "for plot_idx, data_idx in enumerate(top_correct_indices):\n",
    "    ax = axes[0, plot_idx]\n",
    "    img = denormalize(\n",
    "        all_images[data_idx],\n",
    "        config['augmentation']['normalize_mean'],\n",
    "        config['augmentation']['normalize_std']\n",
    "    )\n",
    "    img = torch.clamp(img, 0, 1).permute(1, 2, 0).numpy()\n",
    "    \n",
    "    ax.imshow(img)\n",
    "    ax.set_title(\n",
    "        f'{class_names[all_labels[data_idx]]}\\n{all_confidences[data_idx]*100:.1f}%',\n",
    "        fontsize=10,\n",
    "        fontweight='bold',\n",
    "        color='green'\n",
    "    )\n",
    "    ax.axis('off')\n",
    "\n",
    "# Top incorrect\n",
    "for plot_idx, data_idx in enumerate(top_incorrect_indices):\n",
    "    ax = axes[1, plot_idx]\n",
    "    img = denormalize(\n",
    "        all_images[data_idx],\n",
    "        config['augmentation']['normalize_mean'],\n",
    "        config['augmentation']['normalize_std']\n",
    "    )\n",
    "    img = torch.clamp(img, 0, 1).permute(1, 2, 0).numpy()\n",
    "    \n",
    "    ax.imshow(img)\n",
    "    ax.set_title(\n",
    "        f'T:{class_names[all_labels[data_idx]]}\\nP:{class_names[all_predictions[data_idx]]} ({all_confidences[data_idx]*100:.1f}%)',\n",
    "        fontsize=9,\n",
    "        fontweight='bold',\n",
    "        color='red'\n",
    "    )\n",
    "    ax.axis('off')\n",
    "\n",
    "axes[0, 0].text(-0.3, 0.5, 'Most Confident\\nCorrect', \n",
    "                transform=axes[0, 0].transAxes,\n",
    "                fontsize=14, fontweight='bold',\n",
    "                va='center', ha='right', rotation=90)\n",
    "\n",
    "axes[1, 0].text(-0.3, 0.5, 'Most Confident\\nIncorrect',\n",
    "                transform=axes[1, 0].transAxes,\n",
    "                fontsize=14, fontweight='bold',\n",
    "                va='center', ha='right', rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/confident_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Curves (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if training curves plot exists\n",
    "curves_path = Path('../results/plots/final_training_curves.png')\n",
    "if curves_path.exists():\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(filename=str(curves_path)))\n",
    "    print(\"Training curves loaded from saved plot.\")\n",
    "else:\n",
    "    print(\"Training curves not found. Run full training to generate them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTest Set Accuracy: {accuracy:.2f}%\")\n",
    "print(